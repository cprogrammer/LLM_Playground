{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9623a172-9edd-4206-80d1-baa4656b532c",
   "metadata": {},
   "source": [
    "# Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234e43ba-edcb-4c86-bdfa-da071679ac94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/vlaggeor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (23.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.15.0  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a8999-68a7-4cea-b6c3-766e90088e38",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e66fcc5-c53b-470e-86b3-537af219fe9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b89dec-05e7-4648-af18-410029a7b085",
   "metadata": {},
   "source": [
    "# Load FLAN\n",
    "\n",
    "Load the [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5), creating an instance of the `AutoModelForSeq2SeqLM` class with the `.from_pretrained()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01cecd1a-f6db-44a9-8536-14810d1c8d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29269105-f2f0-4f80-a43f-913db10fc6a8",
   "metadata": {},
   "source": [
    "Download the tokenizer for the FLAN-T5 model using `AutoTokenizer.from_pretrained()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8059a3e-1962-4294-a317-46af28b8fff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385d6498-ed1c-4bf5-ab2c-69e138d6438e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load HuggingFace dialogsum database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34267ab5-9f17-4aff-b5be-8fa1c8ca2fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9fa74-99ec-44d7-a4af-cc2486971da9",
   "metadata": {},
   "source": [
    "# Predict 1, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13589f-1b71-4ddb-9352-102c2fcb3c61",
   "metadata": {},
   "source": [
    "You can change the configuration parameters of the `generate()` method to see a different output from the LLM. So far the only parameter that you have been setting was `max_new_tokens=50`, which defines the maximum number of tokens to generate. A full list of available parameters can be found in the [Hugging Face Generation documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig). \n",
    "\n",
    "Putting the parameter `do_sample = True`, you activate various decoding strategies which influence the next token from the probability distribution over the entire vocabulary. You can then adjust the outputs changing `temperature` and other parameters (such as `top_k` and `top_p`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2874f26f-7673-45c2-9e4b-27955a512456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================\n",
      "Example  1\n",
      "--------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "--------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "--------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING 1:\n",
      "Person1: It's ten to nine.\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING 2:\n",
      "Person1#: It looks like it will take you about 20 minutes if you catch the train\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Person1: It's ten to nine.\n",
      "\n",
      "==========================================================================\n",
      "Example  2\n",
      "--------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "--------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "--------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING 1:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING 2:\n",
      "#Person1#: Thank you, But can you give me a moment to give your\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40, 200]\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "# decode   \n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "# decode arguments\n",
    "    output2 = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=20, do_sample=True, temperature=1.0,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "# prompt\n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "    output_prompt = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )    \n",
    "    print(\"==========================================================================\")\n",
    "    print('Example ', i + 1)\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING 1:\\n{output}\\n')\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING 2:\\n{output2}\\n')\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output_prompt}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd67e12-7dd1-4c93-8502-09b6787d15b2",
   "metadata": {},
   "source": [
    "#  One/ Multi Shot Inference\n",
    "\n",
    "Let's build a function that takes a list of `example_indices_full`, generates a prompt with full examples, then at the end appends the prompt which you want the model to complete (`example_index_to_summarize`).  You will use the same FLAN-T5 prompt template from section [3.2](#3.2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32986ca2-e398-4b15-923a-b7c5d2aee4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb7d89-88b7-42d0-9b3a-84ce8440754f",
   "metadata": {},
   "source": [
    "## Shots test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe594723-e764-4be3-879f-1d65e148a504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "--------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "inputs = tokenizer(shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8b5a6-c65f-4de9-a831-2baf5fa1fd77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
